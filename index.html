<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Entity Extraction - NER with RoBERTa</title>
    <meta name="description" content="Named-Entity Recognition with Fine-tuned RoBERTa Models by brightertiger">
    <meta name="author" content="brightertiger">
    <!-- GitHub Pages specific metadata -->
    <meta name="github-repo" content="entity-extraction">
    <style>
        /* Base styles */
        :root {
            --primary-color: #3498db;
            --secondary-color: #2c3e50;
            --accent-color: #e74c3c;
            --bg-color: #f9f9f9;
            --text-color: #333;
            --code-bg: #f5f5f5;
            --border-color: #ddd;
            --table-alt-bg: #f5f7fa;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--bg-color);
            margin: 0;
            padding: 0;
        }
        
        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            background-color: var(--secondary-color);
            color: white;
            padding: 30px 0;
            margin-bottom: 30px;
        }
        
        header .container {
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        
        nav {
            background-color: var(--primary-color);
            position: sticky;
            top: 0;
            z-index: 100;
        }
        
        nav ul {
            list-style: none;
            padding: 0;
            margin: 0;
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
        }
        
        nav li {
            margin: 0;
            padding: 0;
        }
        
        nav a {
            display: block;
            color: white;
            text-decoration: none;
            padding: 12px 20px;
            transition: background-color 0.2s;
        }
        
        nav a:hover {
            background-color: rgba(255, 255, 255, 0.2);
        }
        
        /* Typography */
        h1 {
            font-size: 2.8rem;
            margin: 0 0 10px 0;
            text-align: center;
        }
        
        h2 {
            font-size: 2rem;
            margin-top: 40px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            color: var(--secondary-color);
        }
        
        h3 {
            font-size: 1.5rem;
            margin-top: 30px;
            color: var(--secondary-color);
        }
        
        h4 {
            font-size: 1.2rem;
            margin-top: 25px;
            color: var(--secondary-color);
        }
        
        p {
            margin: 15px 0;
        }
        
        a {
            color: var(--primary-color);
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        /* Lists */
        ul, ol {
            padding-left: 25px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        /* Code */
        pre, code {
            font-family: 'Fira Mono', 'Courier New', Courier, monospace;
            background-color: var(--code-bg);
            border-radius: 4px;
        }
        
        code {
            padding: 2px 5px;
            font-size: 0.9rem;
        }
        
        pre {
            padding: 15px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
            margin: 20px 0;
        }
        
        pre code {
            padding: 0;
            background-color: transparent;
        }
        
        /* Tables */
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
            overflow-x: auto;
            display: block;
        }
        
        th, td {
            border: 1px solid var(--border-color);
            padding: 10px;
            text-align: left;
        }
        
        th {
            background-color: var(--primary-color);
            color: white;
        }
        
        tr:nth-child(even) {
            background-color: var(--table-alt-bg);
        }
        
        /* Images */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border: 1px solid var(--border-color);
            border-radius: 4px;
        }
        
        /* Specific Components */
        .highlight {
            background-color: #ffeaa7;
            padding: 2px;
            border-radius: 3px;
        }
        
        .terminal {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
            font-family: 'Fira Mono', monospace;
        }
        
        /* Helpers */
        .text-center {
            text-align: center;
        }
        
        .mt-5 {
            margin-top: 50px;
        }
        
        .mb-5 {
            margin-bottom: 50px;
        }
        
        /* Responsive */
        @media (max-width: 768px) {
            nav ul {
                flex-direction: column;
            }
            
            nav a {
                padding: 10px;
                text-align: center;
            }
            
            h1 {
                font-size: 2.2rem;
            }
            
            h2 {
                font-size: 1.8rem;
            }
            
            .container {
                padding: 15px;
            }
        }
        
        /* GitHub link */
        .github-link {
            display: inline-block;
            margin-top: 10px;
            padding: 8px 16px;
            background-color: var(--secondary-color);
            color: white;
            border-radius: 4px;
            text-decoration: none;
            transition: background-color 0.2s;
        }
        
        .github-link:hover {
            background-color: #1a252f;
            text-decoration: none;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Entity Extraction</h1>
            <p>Named-Entity Recognition with Fine-tuned RoBERTa Models</p>
            <a href="https://github.com/brightertiger/entity-extraction" class="github-link" target="_blank">View on GitHub</a>
        </div>
    </header>

    <nav>
        <ul>
            <li><a href="#overview">Overview</a></li>
            <li><a href="#dataset">Dataset</a></li>
            <li><a href="#ml-approach">ML Approach</a></li>
            <li><a href="#code-structure">Code Structure</a></li>
            <li><a href="#instructions">Instructions</a></li>
            <li><a href="#performance">Performance</a></li>
        </ul>
    </nav>

    <div class="container">
        <section id="overview">
            <h2>Overview</h2>
            <p>This repository contains code for Named-Entity Recognition (NER) using finetuned RoBERTa models. The dataset has been borrowed from <a href="https://www.kaggle.com/datasets/namanj27/ner-dataset" target="_blank">Kaggle</a>. The objective is to accurately predict entity tags for words in sentences, distinguishing between different entity types such as people (per), organizations (org), locations (geo), and more.</p>
        </section>

        <section id="dataset">
            <h2>Dataset Structure</h2>
            <p>The input dataset has the following structure:</p>
            <table>
                <thead>
                    <tr>
                        <th>Sentence #</th>
                        <th>Word</th>
                        <th>POS</th>
                        <th>Tag</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Sentence: 1</td>
                        <td>Thousands</td>
                        <td>NNS</td>
                        <td>O</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>of</td>
                        <td>IN</td>
                        <td>O</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>demonstrators</td>
                        <td>NNS</td>
                        <td>O</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>have</td>
                        <td>VBP</td>
                        <td>O</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>marched</td>
                        <td>VBN</td>
                        <td>O</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>through</td>
                        <td>IN</td>
                        <td>O</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>London</td>
                        <td>NNP</td>
                        <td>B-geo</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>to</td>
                        <td>TO</td>
                        <td>O</td>
                    </tr>
                </tbody>
            </table>

            <p>Where:</p>
            <ul>
                <li><strong>Word</strong>: The token from the sentence</li>
                <li><strong>POS</strong>: Part-of-speech tag</li>
                <li><strong>Tag</strong>: The entity tag in IOB format (Inside-Outside-Beginning)
                    <ul>
                        <li><strong>B-</strong> prefix indicates the beginning of an entity</li>
                        <li><strong>I-</strong> prefix indicates the continuation of an entity</li>
                        <li><strong>O</strong> indicates tokens that are not part of any entity</li>
                    </ul>
                </li>
            </ul>
        </section>

        <section id="ml-approach">
            <h2>ML Approach</h2>

            <h3>Model Architecture</h3>
            <p>We use the RoBERTa base model, which is an optimized version of BERT with improved training methodology. RoBERTa features:</p>
            <ul>
                <li>12 transformer layers</li>
                <li>768 hidden dimensions</li>
                <li>12 attention heads</li>
                <li>125M parameters</li>
            </ul>
            <p>For NER, we add a token classification head on top of the RoBERTa encoder, which consists of a dropout layer followed by a linear layer mapping to the number of entity classes.</p>

            <h3>Training Process</h3>
            <p>The model is trained using:</p>
            <ul>
                <li>Cross-entropy loss function</li>
                <li>AdamW optimizer with learning rate of 2e-5</li>
                <li>Linear learning rate scheduler with warmup</li>
                <li>Batch size of 16</li>
                <li>Training runs for 5 epochs</li>
                <li>Gradient accumulation steps of 2</li>
                <li>Max sequence length of 128 tokens</li>
            </ul>

            <h3>Data Preprocessing</h3>
            <ul>
                <li>Sentences are tokenized using RoBERTa's tokenizer</li>
                <li>Special handling for subword tokenization to maintain entity boundaries</li>
                <li>Entity tags are converted to numerical indices using a mapping dictionary</li>
                <li>Dataset is split into training (80%) and validation (20%) sets</li>
            </ul>

            <h3>Evaluation Metrics</h3>
            <p>We evaluate the model using:</p>
            <ul>
                <li><strong>Precision</strong>: Ratio of correctly predicted entities to all predicted entities
                    <ul>
                        <li>Formula: TP / (TP + FP)</li>
                        <li>Measures the accuracy of positive predictions</li>
                        <li>Higher precision means fewer false entity identifications</li>
                    </ul>
                </li>
                <li><strong>Recall</strong>: Ratio of correctly predicted entities to all actual entities
                    <ul>
                        <li>Formula: TP / (TP + FN)</li>
                        <li>Measures the model's ability to find all entities</li>
                        <li>Higher recall means fewer missed entities</li>
                    </ul>
                </li>
                <li><strong>F1 Score</strong>: Harmonic mean of precision and recall
                    <ul>
                        <li>Formula: 2 * (Precision * Recall) / (Precision + Recall)</li>
                        <li>Balances the trade-off between precision and recall</li>
                        <li>Critical for NER tasks where both false positives and false negatives are problematic</li>
                    </ul>
                </li>
                <li><strong>Support</strong>: Number of occurrences of each entity type
                    <ul>
                        <li>Helps interpret metrics in light of class imbalance</li>
                        <li>Low-support classes (like 'art' and 'eve' in our dataset) typically have less reliable metrics</li>
                    </ul>
                </li>
            </ul>

            <p>We report metrics at multiple levels:</p>
            <ul>
                <li><strong>Per-entity metrics</strong>: Performance on each entity type (art, geo, per, etc.)</li>
                <li><strong>Micro average</strong>: Calculated by aggregating contributions of all classes
                    <ul>
                        <li>Favors performance on majority classes</li>
                    </ul>
                </li>
                <li><strong>Macro average</strong>: Simple average of per-class metrics
                    <ul>
                        <li>Each class contributes equally regardless of support</li>
                        <li>Lower macro vs. micro indicates poorer performance on minority classes</li>
                    </ul>
                </li>
                <li><strong>Weighted average</strong>: Average weighted by the support of each class
                    <ul>
                        <li>Reflects overall performance while accounting for class frequency</li>
                    </ul>
                </li>
            </ul>

            <p>For NER specifically, we use a strict match evaluation - an entity prediction is considered correct only if both the entity type and its exact boundary (start and end positions) match the ground truth.</p>
        </section>

        <section id="code-structure">
            <h2>Code Structure</h2>
            <pre><code>├── data                        # DATA FILES
│   ├── data.csv                    # Raw Dataset from Kaggle 
│   ├── train.csv                   # Training split from data.csv
│   └── valid.csv                   # Validation split from data.csv
│   ├── mapping.json                # Mapping of labels to index
│   ├── score.csv                   # Model predictions on valid.csv
│   ├── model                       # Finetuned Model
│   │   └── model.pt-v1.ckpt
│   ├── report.txt                  # Evaluation on valid.csv

├── src                         # SOURCE CODE
│   ├── data.py                     # Data Loaders
│   ├── model.py                    # HuggingFace Model
│   ├── score.py                    # Scoring Model
│   └── train.py                    # Training Model

├── main.py                     
├── conf.yaml</code></pre>
        </section>

        <section id="instructions">
            <h2>Instructions</h2>
            <p>Download the "data.csv" file from Kaggle and place it in data folder.</p>
            <p>Command to split to preprocess data file and split it into training, validation:</p>
            <div class="terminal">python main.py --mode prepare-data</div>

            <p>Command to finetune the model:</p>
            <div class="terminal">python main.py --mode train-model</div>

            <p>Command to evaluate the model:</p>
            <div class="terminal">python main.py --mode score-model</div>
        </section>

        <section id="performance">
            <h2>Performance</h2>
            <p>The final model performance is saved in report.txt file. It looks like:</p>
            <table>
                <thead>
                    <tr>
                        <th>label</th>
                        <th>precision</th>
                        <th>recall</th>
                        <th>f1-score</th>
                        <th>support</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>art</td>
                        <td>0.38</td>
                        <td>0.06</td>
                        <td>0.10</td>
                        <td>161</td>
                    </tr>
                    <tr>
                        <td>eve</td>
                        <td>0.22</td>
                        <td>0.23</td>
                        <td>0.23</td>
                        <td>61</td>
                    </tr>
                    <tr>
                        <td>geo</td>
                        <td>0.86</td>
                        <td>0.88</td>
                        <td>0.87</td>
                        <td>12744</td>
                    </tr>
                    <tr>
                        <td>gpe</td>
                        <td>0.94</td>
                        <td>0.95</td>
                        <td>0.95</td>
                        <td>5020</td>
                    </tr>
                    <tr>
                        <td>nat</td>
                        <td>0.35</td>
                        <td>0.44</td>
                        <td>0.39</td>
                        <td>73</td>
                    </tr>
                    <tr>
                        <td>org</td>
                        <td>0.70</td>
                        <td>0.70</td>
                        <td>0.70</td>
                        <td>6655</td>
                    </tr>
                    <tr>
                        <td>per</td>
                        <td>0.76</td>
                        <td>0.81</td>
                        <td>0.78</td>
                        <td>5195</td>
                    </tr>
                    <tr>
                        <td>tim</td>
                        <td>0.83</td>
                        <td>0.82</td>
                        <td>0.82</td>
                        <td>3942</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td></td>
                        <td></td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>micro avg</td>
                        <td>0.82</td>
                        <td>0.83</td>
                        <td>0.82</td>
                        <td>33851</td>
                    </tr>
                    <tr>
                        <td>macro avg</td>
                        <td>0.63</td>
                        <td>0.61</td>
                        <td>0.60</td>
                        <td>33851</td>
                    </tr>
                    <tr>
                        <td>weighted avg</td>
                        <td>0.82</td>
                        <td>0.83</td>
                        <td>0.82</td>
                        <td>33851</td>
                    </tr>
                </tbody>
            </table>

            <h4>Performance Analysis</h4>
            <p>The performance metrics reveal several important insights:</p>
            <ol>
                <li><strong>Class imbalance impact</strong>:
                    <ul>
                        <li>High-support classes (geo: 12,744, org: 6,655, per: 5,195) achieve F1 scores of 0.87, 0.70, and 0.78 respectively</li>
                        <li>Low-support classes (art: 161, eve: 61, nat: 73) achieve much lower F1 scores of 0.10, 0.23, and 0.39</li>
                    </ul>
                </li>
                <li><strong>Precision-recall balance</strong>:
                    <ul>
                        <li>For most entity types, precision and recall are fairly balanced</li>
                        <li>Exception is 'art' entities where precision (0.38) significantly exceeds recall (0.06), indicating the model is conservative in predicting this class</li>
                    </ul>
                </li>
                <li><strong>Overall performance</strong>:
                    <ul>
                        <li>Micro-average F1 of 0.82 indicates strong general performance</li>
                        <li>Gap between micro-average (0.82) and macro-average (0.60) F1 scores confirms the model performs substantially better on common entity types</li>
                    </ul>
                </li>
                <li><strong>Areas for improvement</strong>:
                    <ul>
                        <li>Targeted strategies for low-resource classes:
                            <ul>
                                <li>Data augmentation for minority classes</li>
                                <li>Class weighting in loss function</li>
                                <li>Few-shot or meta-learning approaches</li>
                            </ul>
                        </li>
                        <li>Entity boundary detection could be improved for complex entities</li>
                    </ul>
                </li>
            </ol>

            <p>The weighted average metrics closely match the micro-average, confirming that performance is dominated by high-frequency classes.</p>

            <h3>Training Progress</h3>
            <p>The wandb logs are below:</p>
            <img src="data/log.png" alt="Training logs from Weights & Biases showing model performance">
        </section>

        <footer class="mt-5 text-center">
            <p>Named-Entity Recognition with RoBERTa | <a href="https://github.com/brightertiger/entity-extraction" target="_blank">GitHub Repository</a></p>
            <p>&copy; 2023 <a href="https://github.com/brightertiger" target="_blank">brightertiger</a></p>
        </footer>
    </div>
</body>
</html> 